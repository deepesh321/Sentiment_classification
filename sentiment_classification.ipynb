{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPedQSZ90qi1RbuZ7ST3Zr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepesh321/Sentiment_classification/blob/master/sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEfA-15Lw3w1",
        "colab_type": "code",
        "outputId": "a436b668-a743-44c0-fcfa-203afa820b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaASdqEExk9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR='/content/drive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afZSs0APxv7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df =pd.read_csv(BASE_DIR+'text_train.csv',names=[\n",
        "                 'image_name', 'funny_image', 'sarcastic_image', 'offensive_image', 'motivational_image', 'text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-4GF1IqN2s5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "7dd693b5-4156-4c79-ad62-b36c2f9a6ab3"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>funny_image</th>\n",
              "      <th>sarcastic_image</th>\n",
              "      <th>offensive_image</th>\n",
              "      <th>motivational_image</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10_year_2r94rv.jpg</td>\n",
              "      <td>funny</td>\n",
              "      <td>sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10_year_10-year-challenge_1547788782.jpeg</td>\n",
              "      <td>not_funny</td>\n",
              "      <td>sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10_year_10yearchallenge-5c75f8b946e0fb0001edc7...</td>\n",
              "      <td>funny</td>\n",
              "      <td>not_sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image_name  ...                                               text\n",
              "0                                 10_year_2r94rv.jpg  ...  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...\n",
              "1          10_year_10-year-challenge_1547788782.jpeg  ...  The best of #10 YearChallenge! Completed in le...\n",
              "2  10_year_10yearchallenge-5c75f8b946e0fb0001edc7...  ...  Sam Thorne @Strippin ( Follow Follow Saw every...\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvZgWxdFOKDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    # Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "\n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSJyAS1mOMcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb2009a7-99f5-463d-eda7-f77b28062180"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "df['text'] = df['text'].map(lambda x: clean_text(x))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4GPzbLTO0qO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "2f68a255-0afe-4e86-8df7-f830948eebac"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>funny_image</th>\n",
              "      <th>sarcastic_image</th>\n",
              "      <th>offensive_image</th>\n",
              "      <th>motivational_image</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10_year_2r94rv.jpg</td>\n",
              "      <td>funny</td>\n",
              "      <td>sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>look friend lightyear sohalikut trend play yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10_year_10-year-challenge_1547788782.jpeg</td>\n",
              "      <td>not_funny</td>\n",
              "      <td>sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>best 10 yearchalleng ! complet less year kudus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10_year_10yearchallenge-5c75f8b946e0fb0001edc7...</td>\n",
              "      <td>funny</td>\n",
              "      <td>not_sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>sam thorn strippin follow follow saw everyon p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image_name  ...                                               text\n",
              "0                                 10_year_2r94rv.jpg  ...  look friend lightyear sohalikut trend play yea...\n",
              "1          10_year_10-year-challenge_1547788782.jpeg  ...  best 10 yearchalleng ! complet less year kudus...\n",
              "2  10_year_10yearchallenge-5c75f8b946e0fb0001edc7...  ...  sam thorn strippin follow follow saw everyon p...\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq9Msy1qO5Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "for index, row in df.iterrows():\n",
        "    if row['funny_image'] == 'funny':\n",
        "        row['funny_image'] = 1\n",
        "    else:\n",
        "        row['funny_image'] = 0\n",
        "    if row['sarcastic_image'] == 'sarcastic':\n",
        "        row['sarcastic_image'] = 1\n",
        "    else:\n",
        "        row['sarcastic_image'] = 0\n",
        "    if row['offensive_image'] == 'offensive':\n",
        "        row['offensive_image'] = 1\n",
        "    else:\n",
        "        row['offensive_image'] = 0\n",
        "    if row['motivational_image'] == 'motivational':\n",
        "        row['motivational_image'] = 1\n",
        "    else:\n",
        "        row['motivational_image'] = 0\n",
        "    labels.append([row['funny_image'], row['sarcastic_image'],\n",
        "                   row['offensive_image'], row['motivational_image']])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uR7UiU2PQk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oabjYRAIP9-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this approach we learn word embedding as part of fitting a neural network model.\n",
        "tokenizer_obj=Tokenizer()\n",
        "#print(tokenizer_obj)\n",
        "tokenizer_obj.fit_on_texts(df['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYua7IqrR4W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length=max([len(s.split()) for s in df['text']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PDPPoQpTCKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(tokenizer_obj.word_index)+1\n",
        "#print(tokenizer_obj.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLLiJbabWsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_tokens=tokenizer_obj.texts_to_sequences(df['text'])\n",
        "data=pad_sequences(data_tokens,maxlen=max_length,padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTCzDFzac3vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import SGD\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcdiyA40dw4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,100,input_length=max_length))\n",
        "model.add(GRU(units=64,dropout=0.2))\n",
        "model.add(Dense(4,activation='sigmoid'))\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 10))\n",
        "opt = SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rtoPwLoSLGR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1f95c8bc-6cca-4c1b-e91b-2565e1ea3591"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1316, 100)         1111000   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 64)                31680     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 1,142,940\n",
            "Trainable params: 1,142,940\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28jyBq4Hn-xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "71658887-5301-49db-a65c-568db6fd0194"
      },
      "source": [
        "model.fit(data,np.array(labels),validation_split=0.1,callbacks=[lr_schedule])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6822 samples, validate on 759 samples\n",
            "Epoch 1/1\n",
            "6822/6822 [==============================] - 244s 36ms/step - loss: 0.7031 - acc: 0.4039 - val_loss: 0.7011 - val_acc: 0.4651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3bca24898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gGH2TE85AQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQkHt0D05NK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this method we first separately learn word embeddings and then pass to the embedding layer.\n",
        "data=[]\n",
        "data1=df['text'].values.tolist()\n",
        "#print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD__YaoK5fE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71504eb9-2cd2-4937-c160-34864af2a4a8"
      },
      "source": [
        "nltk.download('punkt')\n",
        "for line in data1:\n",
        "  tokens=word_tokenize(line)\n",
        "  tokens=[w.lower() for w in tokens]\n",
        "  #table=str.maketrans('','',string.punctuation)\n",
        "  stripped=[w.translate(tokens) for w in tokens]\n",
        "  words=[word for word in stripped if word.isalpha()]\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  words=[w for w in words if not w in stop_words]\n",
        "  data.append(words)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D_etc5e_r6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPIu8x40Aau7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "model=gensim.models.Word2Vec(sentences=data,size=100,window=5,workers=4,min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8JNBphxBVKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=list(model.wv.vocab)\n",
        "#print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJGnxBjetXYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "74c1f90c-33d3-4ca4-a112-9580c99710de"
      },
      "source": [
        "filename=BASE_DIR+'word2vec.txt'\n",
        "model.wv.save_word2vec_format(filename,binary=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooGszKlYDdIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index={}\n",
        "f=open(filename)\n",
        "for line in f:\n",
        "  values=line.split()\n",
        "  word=values[0]\n",
        "  coefs=np.asarray(values[1:])\n",
        "  embeddings_index[word]=coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiWsB9gEXF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_obj=Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(data)\n",
        "sequences=tokenizer_obj.texts_to_sequences(data)\n",
        "data_pad=pad_sequences(sequences,maxlen=max_length,padding='post')\n",
        "word_index=tokenizer_obj.word_index\n",
        "#print(word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKHlBlzLGUT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index), 100))\n",
        "for word, index in word_index.items():\n",
        "    if index > len(word_index) - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "#print(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOY7-CVHN9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.initializers import Constant\n",
        "model=Sequential()\n",
        "embedding_layer=Embedding(vocab_size,100,embeddings_initializer=Constant(embedding_matrix),input_length=max_length,trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=64,dropout=0.2))\n",
        "model.add(Dense(4,activation='sigmoid'))\n",
        "lr_schedule = keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 10))\n",
        "opt = SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Axs1HjYGWlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "645545aa-cdbf-460d-da74-39c82991ddd6"
      },
      "source": [
        "model.fit(data_pad,np.array(labels),validation_split=0.1,callbacks=[lr_schedule])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6822 samples, validate on 759 samples\n",
            "Epoch 1/1\n",
            "6822/6822 [==============================] - 210s 31ms/step - loss: 0.6931 - acc: 0.6795 - val_loss: 0.6931 - val_acc: 0.6887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3b71188d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}